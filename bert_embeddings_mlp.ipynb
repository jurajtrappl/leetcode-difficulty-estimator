{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb58543-b5d0-4dc0-8a2f-dbfb263899b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU keras_tuner beautifulsoup4 matplotlib transformers tqdm seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ffddf",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845e911-d47e-4eb8-a5e6-34ed6852f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import json\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, CosineDecay\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850a356-2412-4a15-aa29-de76a2f9c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"second_layer_mean_pooling_dense\"\n",
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4083",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89dc9eb-9ac8-4377-a2ad-b9c44c73484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"leetcode_problems_dataset.json\", \"r\") as f:\n",
    "    problems = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08e0e9-441d-4985-b0be-96e8199e6476",
   "metadata": {},
   "source": [
    "Premium problems don't have content (since I'm not a premium user) and get rid of HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87543faa-d7fe-4d56-8feb-f43a51952569",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_without_html = []\n",
    "for problem_name, problem_data in tqdm(problems.items()):\n",
    "    if not problem_data[\"content\"]:\n",
    "        continue\n",
    "    \n",
    "    problems_without_html.append((BeautifulSoup(problem_data[\"content\"], \"html.parser\").get_text(), problem_data[\"difficulty\"]))\n",
    "    \n",
    "X, y = [], []\n",
    "difficulties_int = { \"Easy\": 0, \"Medium\": 1, \"Hard\": 2 }\n",
    "for problem_description, difficulty in problems_without_html:\n",
    "    X.append(problem_description)\n",
    "    y.append(difficulties_int[difficulty])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935796c6-1369-440f-83ff-68db208e7325",
   "metadata": {},
   "source": [
    "## BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c417c-23c7-491a-b6a4-cbfa5b4d40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea70442-cc02-4c1f-95cb-1bce6751219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_tf(text, pooling, layer_number):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(inputs, output_hidden_states=True)\n",
    "    layer_hidden_states = outputs.hidden_states[layer_number]\n",
    "\n",
    "    if pooling == \"mean\":\n",
    "        return tf.reduce_mean(layer_hidden_states, axis=1)\n",
    "    elif pooling == \"max\":\n",
    "        return tf.reduce_max(layer_hidden_states, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102bdb7-ad8a-48e8-988d-52e2d9922626",
   "metadata": {},
   "source": [
    "Create the embeddings, specify the layer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f42acf-a5df-4791-ac07-e2949011d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for problem_description in tqdm(X):\n",
    "    embedding_tensor = get_bert_embeddings_tf(problem_description, \"mean\", 1)\n",
    "    embedding_array = embedding_tensor.numpy().squeeze()\n",
    "    embeddings.append(embedding_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fac3c3-d7d2-4237-a030-a8bc450375af",
   "metadata": {},
   "source": [
    "### Intermezzo: Embedding projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafb8ff-c156-46c5-87af-b7a087ac819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df.to_csv(f\"{EXPERIMENT_NAME}_embeddings.tsv\", sep='\\t', index=False, header=False)\n",
    "\n",
    "problem_names = [problem_name for problem_name in problems if problems[problem_name][\"content\"]]\n",
    "target_class = [problems[problem_name][\"difficulty\"] for problem_name in problems if problems[problem_name][\"content\"]]\n",
    "metadata_df = pd.DataFrame({'Text Label': problem_names, 'Target Label': target_class})\n",
    "metadata_df.to_csv(f\"{EXPERIMENT_NAME}_embeddings_metadata.tsv\", sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f829e2-2089-4867-bac4-88aacc18db72",
   "metadata": {},
   "source": [
    "## Build train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b3734-ad9e-411d-87ee-7bf94be1d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_dataset(features, labels):\n",
    "    \"\"\"\n",
    "    Downsamples the dataset to have an equal distribution of classes.\n",
    "\n",
    "    Parameters:\n",
    "    X (list): Feature data.\n",
    "    y (list): Corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Downsampled feature data and labels.\n",
    "    \"\"\"\n",
    "    paired_data = list(zip(features, labels))\n",
    "    class_distribution = Counter(labels)\n",
    "\n",
    "    min_samples = min(class_distribution.values())\n",
    "\n",
    "    downsampled_data = []\n",
    "    class_counts = {cls: 0 for cls in class_distribution.keys()}\n",
    "\n",
    "    for data, label in paired_data:\n",
    "        if class_counts[label] < min_samples:\n",
    "            downsampled_data.append((data, label))\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    features_downsampled, labels_downsampled = zip(*downsampled_data)\n",
    "\n",
    "    return list(features_downsampled), list(labels_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821d757-cf79-4e31-8fe7-da5a09652968",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = downsample_dataset(embeddings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f785c-95dc-4e67-880d-d1649782df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=SEED, stratify=y_val)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a81c4-1b7d-43e0-9f9c-6363b23747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12fe48",
   "metadata": {},
   "source": [
    "## Build Dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c09a1-2e77-4940-af3c-b6856caae36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(input, units, activation, l1_value, l2_value, dropout_rate, use_batch_norm):\n",
    "    hidden = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value)\n",
    "    )(input)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        \n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(hidden)\n",
    "    \n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33429325-69fe-4bd1-b083-011a61086218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # x = tf.keras.layers.Reshape((input_shape[0], 1))(input_layer)\n",
    "    \n",
    "    l1_value = hp.Float('l1', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    l2_value = hp.Float('l2', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # for i in range(hp.Int('num_conv_blocks', 1, 5)):\n",
    "    #    num_filters = hp.Int(f'filters_{i+1}', 16, 512, step=16)\n",
    "    #    kernel_size = hp.Int(f'kernel_size_{i+1}', 3, 7)\n",
    "    #    x = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu', kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value))(x)\n",
    "    #    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    #    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    #    x = tf.keras.layers.Dropout(rate=hp.Float('conv_dropout', min_value=0.0, max_value=0.5))(x)\n",
    "\n",
    "    # x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    hp_number_of_hidden_layers = hp.Int('number_of_hidden_layers', 1, 3)\n",
    "    hidden = input\n",
    "    for i in range(hp_number_of_hidden_layers):\n",
    "        hidden = dense_block(\n",
    "            hidden,\n",
    "            units=hp.Int(f'units_{i+1}', min_value=32, max_value=4096, step=32),\n",
    "            activation=hp.Choice(f'activation_{i+1}', values=['relu', 'elu', 'selu', 'tanh']),\n",
    "            l1_value=l1_value,\n",
    "            l2_value=l2_value,\n",
    "            dropout_rate=hp.Float('dropout', min_value=0.0, max_value=0.5, default=0.25, step=0.05),\n",
    "            use_batch_norm=hp.Boolean('use_batch_norm', default=False)\n",
    "        )\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=3, activation=tf.nn.softmax)(hidden)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "    \n",
    "    hp_initial_lr = hp.Choice('initial_lr', [1e-2, 1e-3, 1e-4])\n",
    "    hp_sgd_momentum = hp.Choice('sgd_momentum', values=[.8, .9])\n",
    "    hp_optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
    "    if hp_optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_initial_lr)\n",
    "    elif hp_optimizer_name == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=hp_initial_lr, momentum=hp_sgd_momentum, nesterov=True)\n",
    "    elif hp_optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=hp_initial_lr)\n",
    "    elif hp_optimizer_name == 'adagrad':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=hp_initial_lr)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4262929-3bc2-4a0c-a107-60ba458b93c9",
   "metadata": {},
   "source": [
    "## Train & optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923de2-028f-48d6-9b6c-89948e215d43",
   "metadata": {},
   "source": [
    "Set up hyper search using the hyper band algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcd5b6-adde-4092-a606-a49202fa6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_sparse_categorical_accuracy',\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=1,\n",
    "    directory=EXPERIMENT_NAME,\n",
    "    project_name='lc-difficulty-estimator',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3843d4e-10b9-4667-89cc-7a04b3b054f6",
   "metadata": {},
   "source": [
    "Start search for the best hyperparameters given the current datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86850d6b-c5b4-4959-82ba-660765c42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "           CosineDecay(1e-3, X_train.shape[0] // BATCH_SIZE * 100)\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ee5bb-40a5-418b-a64a-f9dd35e84182",
   "metadata": {},
   "source": [
    "Get the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69304ac-97f1-417a-8a70-0f4afeb76d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4ac75-59ab-42d6-b3ba-0f0207d25330",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{EXPERIMENT_NAME}_hps.json\", \"w\") as f:\n",
    "    json.dump(best_hps.values, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94981f88-5ceb-4b07-81f5-8545fb825f11",
   "metadata": {},
   "source": [
    "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da94072-d9e1-44a7-8781-a7564eee958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_model = tuner.hypermodel.build(best_hps)\n",
    "best_hps_history = best_hps_model.fit(\n",
    "    train,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "            CosineDecay(1e-3, X_train.shape[0] // BATCH_SIZE * 100)\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = best_hps_history.history['val_sparse_categorical_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "f\"Best epoch: {best_epoch}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef849e-e007-416b-9df2-9bf45d36b346",
   "metadata": {},
   "source": [
    "Re-instantiate the hypermodel and train it with the optimal number of epochs from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dfbcc-a278-4974-bf7e-66f03def611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "hypermodel_history = hypermodel.fit(\n",
    "    train,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "            CosineDecay(1e-3, X_train.shape[0] // BATCH_SIZE * 100)\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766a92e-c1f0-40d2-845e-b23091983935",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e543f-530d-49a5-95fe-f2d32aeeec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca40f9-9da8-4447-a57d-eba30d9d85ef",
   "metadata": {},
   "source": [
    "Look at the distributions of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee24e9-37a1-4e15-8005-078062ad4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = hypermodel.predict(X_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeeb41-17f9-48ce-89a3-2dbd3c9a3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel.save(f\"{EXPERIMENT_NAME}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0873eb-6864-467b-917b-0bf744a530e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "class_labels = ['Easy', 'Medium', 'Hard']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='BuGn', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(f'Confusion Matrix for {EXPERIMENT_NAME}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(f\"{EXPERIMENT_NAME}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb34c6-f53c-4f38-a04d-d9e328ea5fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be7891-5d0b-40fa-9a11-ff2273e1b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
