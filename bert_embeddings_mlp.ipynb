{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb58543-b5d0-4dc0-8a2f-dbfb263899b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU keras_tuner nltk beautifulsoup4 matplotlib imbalanced-learn transformers tqdm seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ffddf",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845e911-d47e-4eb8-a5e6-34ed6852f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import json\n",
    "import keras_tuner as kt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay, CosineDecay\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850a356-2412-4a15-aa29-de76a2f9c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4083",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89dc9eb-9ac8-4377-a2ad-b9c44c73484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('leetcode_problems_dataset.json', 'r') as f:\n",
    "    problems = json.load(f)\n",
    "    \n",
    "len(problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73756907-6133-4025-be1b-37fc3ce0b5a5",
   "metadata": {},
   "source": [
    "Get rid of HTML tags and extract just the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e91eaa-1945-454a-916a-49baac2285c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems[\"two-sum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08e0e9-441d-4985-b0be-96e8199e6476",
   "metadata": {},
   "source": [
    "Premium problems don't have content (since I'm not a premium user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87543faa-d7fe-4d56-8feb-f43a51952569",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_without_html = []\n",
    "for problem_name, problem_data in tqdm(problems.items()):\n",
    "    if not problem_data[\"content\"]:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        problems_without_html.append((BeautifulSoup(problem_data[\"content\"], \"html.parser\").get_text(), problem_data[\"difficulty\"]))\n",
    "    except Exception as e:\n",
    "        print(problem_name)\n",
    "    \n",
    "len(problems_without_html)\n",
    "problems_without_html[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c417c-23c7-491a-b6a4-cbfa5b4d40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9bba5-b85b-4400-89af-1a87de770331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_tf(text, pooling):\n",
    "    # use bert tokenizer to tokenize the text in a way bert can then work with it\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # - generate embeddings - bert generates output of shape [batch_size, sequence_length, hidden_size]\n",
    "    # - batch size is 1 because we use only one description, sequence length is max_length and hidden size is from bert 768\n",
    "    # (1, 512, 768)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    if pooling == \"mean\":\n",
    "        # averages the embeddings across all tokens in sequence\n",
    "        return tf.reduce_mean(last_hidden_states, axis=1)\n",
    "    elif pooling == \"max\":\n",
    "        # Takes the maximum value across each dimension of the token embeddings. Useful for capturing the most salient features in the text.\n",
    "        return tf.reduce_max(last_hidden_states, axis=1)\n",
    "    elif pooling == \"cls\":\n",
    "        # Uses the embedding of the special [CLS] token, which is typically used by BERT for classification tasks. This token is designed to capture the overall context of the entire sequence.\n",
    "        return last_hidden_states[:, 0, :]\n",
    "    elif pooling == \"cls+mean\":\n",
    "        # Combines the [CLS] token embedding with mean-pooled embeddings to capture both overall context and detailed features.\n",
    "        mean_pooled = tf.reduce_mean(last_hidden_states, axis=1)\n",
    "        cls_embedding = last_hidden_states[:, 0, :]\n",
    "        return tf.concat([cls_embedding, mean_pooled], axis=1)\n",
    "    \n",
    "X = []\n",
    "for text, _ in tqdm(problems_without_html):\n",
    "    embedding_tensor = get_bert_embeddings_tf(text, \"mean\")\n",
    "    embedding_array = embedding_tensor.numpy().squeeze()  # Convert to numpy array and remove the first dimension\n",
    "    X.append(embedding_array)\n",
    "\n",
    "random.choice(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fac3c3-d7d2-4237-a030-a8bc450375af",
   "metadata": {},
   "source": [
    "### Intermezzo: Embedding projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafb8ff-c156-46c5-87af-b7a087ac819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings_df = pd.DataFrame(X)\n",
    "embeddings_df.to_csv('bert_embeddings_mean.tsv', sep='\\t', index=False, header=False)\n",
    "\n",
    "# Metadata for better visualizations\n",
    "problem_names = [problem_name for problem_name in problems if problems[problem_name][\"content\"]]\n",
    "target_class = [problems[problem_name][\"difficulty\"] for problem_name in problems if problems[problem_name][\"content\"]]\n",
    "metadata_df = pd.DataFrame({'Text Label': problem_names, 'Target Label': target_class})\n",
    "metadata_df.to_csv('metadata_mean.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14ab13-be5f-4370-9b94-d5223f95e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulties_int = { \"Easy\": 0, \"Medium\": 1, \"Hard\": 2 }\n",
    "y = [difficulties_int[difficulty] for _, difficulty in problems_without_html]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7870a21",
   "metadata": {},
   "source": [
    "Generate balanced batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f785c-95dc-4e67-880d-d1649782df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70:15:15 train:validation:test, stratify is there to preserve the real world ratio in validation and test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=SEED, stratify=y_val)\n",
    "\n",
    "# for convenience\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)\n",
    "print(f\"{len(X_train)=}, {len(X_val)=}, {len(X_test)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7573ebb8-490f-4d02-8fd5-c0ae24927964",
   "metadata": {},
   "source": [
    "Oversampling technique of the minority class, common approach to handle imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419792d8-d860-4c0c-bd06-03ee0b67eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote = SMOTE(random_state=SEED)\n",
    "\n",
    "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(kind='borderline-1', random_state=SEED)\n",
    "X_train_smote, y_train_smote = borderline_smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a81c4-1b7d-43e0-9f9c-6363b23747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train_smote, y_train_smote)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validation = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12fe48",
   "metadata": {},
   "source": [
    "## Build Dense network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c09a1-2e77-4940-af3c-b6856caae36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "def dense_block(input, units, activation, l1_value, l2_value, dropout_rate, use_batch_norm):\n",
    "    hidden = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value)\n",
    "    )(input)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        \n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(hidden)\n",
    "    \n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33429325-69fe-4bd1-b083-011a61086218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "def build_model(hp):\n",
    "    input_shape = (X_train_smote.shape[1],)\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Reshape for Conv1D: Input shape should be (batch_size, steps, input_dim)\n",
    "    x = tf.keras.layers.Reshape((input_shape[0], 1))(input_layer)\n",
    "    \n",
    "    l1_value = hp.Float('l1', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    l2_value = hp.Float('l2', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    for i in range(hp.Int('num_conv_blocks', 1, 5)):\n",
    "        num_filters = hp.Int(f'filters_{i+1}', 16, 512, step=16)\n",
    "        kernel_size = hp.Int(f'kernel_size_{i+1}', 3, 7)\n",
    "        x = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu', kernel_regularizer=l1_l2(l1=l1_value, l2=l2_value))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = tf.keras.layers.Dropout(rate=hp.Float('conv_dropout', min_value=0.0, max_value=0.5))(x)\n",
    "\n",
    "    # Flatten the output of the conv layers to feed into dense layers\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    hp_number_of_hidden_layers = hp.Int('number_of_hidden_layers', 1, 3)\n",
    "    hidden = x\n",
    "    for i in range(hp_number_of_hidden_layers):\n",
    "        hidden = dense_block(\n",
    "            hidden,\n",
    "            units=hp.Int(f'units_{i+1}', min_value=32, max_value=1024, step=32),\n",
    "            activation=hp.Choice(f'activation_{i+1}', values=['relu', 'elu', 'selu', 'tanh']),\n",
    "            l1_value=l1_value,\n",
    "            l2_value=l2_value,\n",
    "            dropout_rate=hp.Float('dropout', min_value=0.0, max_value=0.5, default=0.25, step=0.05),\n",
    "            use_batch_norm=hp.Boolean('use_batch_norm', default=False)\n",
    "        )\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=3, activation=tf.nn.softmax)(hidden)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    # choosing optimizer\n",
    "    hp_initial_lr = hp.Choice('initial_lr', [1e-2, 1e-3, 1e-4])\n",
    "    hp_sgd_momentum = hp.Choice('sgd_momentum', values=[.8, .9])\n",
    "    hp_optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
    "    if hp_optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_initial_lr)\n",
    "    elif hp_optimizer_name == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=hp_initial_lr, momentum=hp_sgd_momentum, nesterov=True)\n",
    "    elif hp_optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=hp_initial_lr)\n",
    "    elif hp_optimizer_name == 'adagrad':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=hp_initial_lr)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4262929-3bc2-4a0c-a107-60ba458b93c9",
   "metadata": {},
   "source": [
    "## Train MLP classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923de2-028f-48d6-9b6c-89948e215d43",
   "metadata": {},
   "source": [
    "Set up hyper search using the hyper band algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcd5b6-adde-4092-a606-a49202fa6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_sparse_categorical_accuracy',\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=1,\n",
    "    directory='mlp_be_REST_OF_THE_NAME_HERE',\n",
    "    project_name='mlp_classification',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3843d4e-10b9-4667-89cc-7a04b3b054f6",
   "metadata": {},
   "source": [
    "Start search for the best hyperparameters given the current datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86850d6b-c5b4-4959-82ba-660765c42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "           CosineDecay(1e-3, X_train_smote.shape[0] // BATCH_SIZE * 100)\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ee5bb-40a5-418b-a64a-f9dd35e84182",
   "metadata": {},
   "source": [
    "Get the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9fdfd-b5e1-403b-a3c9-c5e09c9d1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94981f88-5ceb-4b07-81f5-8545fb825f11",
   "metadata": {},
   "source": [
    "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da94072-d9e1-44a7-8781-a7564eee958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps_model = tuner.hypermodel.build(best_hps)\n",
    "best_hps_history = best_hps_model.fit(\n",
    "    train,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "            CosineDecay(1e-3, X_train_smote.shape[0] // BATCH_SIZE * 100)\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")\n",
    "\n",
    "val_acc_per_epoch = best_hps_history.history['val_sparse_categorical_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "f\"Best epoch: {best_epoch}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef849e-e007-416b-9df2-9bf45d36b346",
   "metadata": {},
   "source": [
    "Re-instantiate the hypermodel and train it with the optimal number of epochs from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dfbcc-a278-4974-bf7e-66f03def611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "hypermodel_history = hypermodel.fit(\n",
    "    train,\n",
    "    epochs=best_epoch,\n",
    "    callbacks=[\n",
    "        # tf.keras.callbacks.LearningRateScheduler(\n",
    "        #    CosineDecay(1e-3, X_train_smote.shape[0] // BATCH_SIZE * 100)\n",
    "        # ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766a92e-c1f0-40d2-845e-b23091983935",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e543f-530d-49a5-95fe-f2d32aeeec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca40f9-9da8-4447-a57d-eba30d9d85ef",
   "metadata": {},
   "source": [
    "Look at the distributions of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee24e9-37a1-4e15-8005-078062ad4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = hypermodel.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbeeb41-17f9-48ce-89a3-2dbd3c9a3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel.save('mlp_be_REST_OF_THE_NAME_HERE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f60ad-9f95-4ebe-b340-5086f120f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "class_labels = ['Easy', 'Medium', 'Hard']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eb705-9aac-46a5-8c39-0c4259bb9983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ea024-c92c-4487-bfa3-05331255f374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c888c-4edd-40ae-a992-73f8ee2e7a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
