{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e4c02-46bc-4fa8-bff9-f347c760a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU keras_tuner missingno matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ffddf",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845e911-d47e-4eb8-a5e6-34ed6852f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "\n",
    "from leetcode_dataset import LeetcodeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850a356-2412-4a15-aa29-de76a2f9c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4083",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89dc9eb-9ac8-4377-a2ad-b9c44c73484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = LeetcodeDataset().dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4d972",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c6a17-1091-4a86-a5ce-251e243a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(preprocessor=None):\n",
    "    corpus = df[\"explanation\"].values\n",
    "\n",
    "    # tokenize text\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r'\\b\\w+\\b|`[^`]+`|\\S', # words, numbers, symbols, and code-like elements\n",
    "    )\n",
    "    tf_idf_explanations = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # scale the data (- mean, div by std)\n",
    "    scaler = StandardScaler()\n",
    "    tf_idf_explanations_dense_scaled = scaler.fit_transform(tf_idf_explanations.toarray())\n",
    "\n",
    "    if not preprocessor:\n",
    "        return tf_idf_explanations_dense_scaled, df[\"difficulty int\"].values\n",
    "    \n",
    "    tf_idf_explanations_preproc = preprocessor.fit_transform(tf_idf_explanations_dense_scaled)\n",
    "    \n",
    "    return tf_idf_explanations_preproc, df[\"difficulty int\"].values\n",
    "\n",
    "# PCA init is more stable (docs)\n",
    "# play with perplexity 5-50, i saw sklearn doc example for 1500 examples and it was set to 30 (roughly our number of samples)\n",
    "# tsne = TSNE(n_components=3, perplexity=30, init=\"pca\", learning_rate=\"auto\", random_state=SEED)\n",
    "# X, y = preprocess_data(tsne)\n",
    "\n",
    "# pca = PCA(n_components=100, random_state=SEED)\n",
    "X, y = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7870a21",
   "metadata": {},
   "source": [
    "Generate balanced batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f785c-95dc-4e67-880d-d1649782df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:10:10 train:validation:test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"difficulty int\"].values\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_val\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)\n",
    "print(f\"{len(X_train)=}, {len(X_val)=}, {len(X_test)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44cdda-f21c-4cf8-9a98-a2fe28a36a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_batches_bootstrap(X, y, batch_size, n_classes, n_batches):\n",
    "    for _ in range(n_batches):\n",
    "        X_batch = np.zeros((batch_size, X.shape[1]))\n",
    "        y_batch = np.zeros(batch_size)\n",
    "        \n",
    "        samples_per_class = batch_size // n_classes\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            class_indices = np.where(y == i)[0]\n",
    "            \n",
    "            chosen_indices = resample(class_indices, n_samples=samples_per_class, replace=True)\n",
    "            \n",
    "            X_batch[i * samples_per_class:(i + 1) * samples_per_class] = X[chosen_indices]\n",
    "            y_batch[i * samples_per_class:(i + 1) * samples_per_class] = y[chosen_indices]\n",
    "        \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE, NUMBER_OF_BATCHES = 30, 30\n",
    "\n",
    "train = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_balanced_batches_bootstrap(X_train, y_train, BATCH_SIZE, 3, NUMBER_OF_BATCHES),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=([BATCH_SIZE, X_train.shape[1]], [BATCH_SIZE])\n",
    ")\n",
    "\n",
    "validation = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_balanced_batches_bootstrap(X_val, y_val, 15, 3, 15),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=([15, X_val.shape[1]], [15])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12fe48",
   "metadata": {},
   "source": [
    "## Build MLP\n",
    "\n",
    "Implement a function that creates dense layer with batchnorm + dropout regularizations. Parameters are feeded from hypersearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96957649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(input, units, activation, l2, dropout_rate):\n",
    "    hidden = tf.keras.layers.Dense(\n",
    "        units=units,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(l2)\n",
    "    )(input)\n",
    "    batch_norm = tf.keras.layers.BatchNormalization()(hidden)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(batch_norm)\n",
    "    \n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f489d39-207a-47e3-ab1e-c4c8002dae2f",
   "metadata": {},
   "source": [
    "Define a function that builds MLP and fills hyperparams using the keras tuner hyper search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc1e37-9c56-4a37-85ae-411f20ac6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    hp_number_of_hidden_layers = hp.Int('number_of_hidden_layers', 1, 5)\n",
    "    hidden = input\n",
    "    for i in range(hp_number_of_hidden_layers):\n",
    "        hp_units = hp.Int(f'units_{i+1}', min_value=32, max_value=1024, step=32)\n",
    "        hp_l2 = hp.Choice(f'l2_{i+1}', values=[.1, .01, .001, .0001])\n",
    "        hp_dropout = hp.Choice(f'dropout_{i+1}', values=[.2, .3, .4, .5])\n",
    "        hp_activation = hp.Choice(f'activation_{i+1}', values=['relu', 'elu', 'selu', 'tanh'])\n",
    "        hidden = dense_block(hidden, units=hp_units, activation=hp_activation, l2=hp_l2, dropout_rate=hp_dropout)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=3, activation=tf.nn.softmax)(hidden)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[.05, .01, .005, .001, .0005, .0001])\n",
    "    hp_sgd_momentum = hp.Choice('sgd_momentum', values=[.8, .9])\n",
    "    hp_optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
    "\n",
    "    if hp_optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer_name == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=hp_learning_rate, momentum=hp_sgd_momentum, nesterov=True)\n",
    "    elif hp_optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer_name == 'adagrad':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=hp_learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923de2-028f-48d6-9b6c-89948e215d43",
   "metadata": {},
   "source": [
    "Set up hyper search using the hyper band algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86850d6b-c5b4-4959-82ba-660765c42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_sparse_categorical_accuracy',\n",
    "    directory='mlp_model_all_tfidf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffac1c-4f75-44af-9304-c12d4d1a3c72",
   "metadata": {},
   "source": [
    "### Find best hyperparams for the MLP\n",
    "\n",
    "Start search for the best hyperparameters given the current datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0219b-65f1-426a-a08a-3abb3b678645",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e157af7-c03e-4a44-8d93-8b9e6932042e",
   "metadata": {},
   "source": [
    "Extract hyperparameters from the best trial and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d148b6-fdb6-4f67-ab07-81b24e45ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_hps = best_trial.hyperparameters\n",
    "\n",
    "with open('mlp_all_tfidf_best_hp.json', 'w') as f:\n",
    "    json.dump(best_hps.values, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7868bc5-ef4d-4b90-91b8-53fbd557ef32",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Build the best model using the best hyperparameters, save the model and print its tensorflow summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e713a-f953-4ed8-b72a-d550114951dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(best_hps)\n",
    "best_model.save('mlp_all_tfidf_best.h5')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3831ac-ca52-48b6-bba3-542ef9b21b11",
   "metadata": {},
   "source": [
    "Lets see the loss and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1387138-202b-4399-9ddd-560b7e208ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7da75-6154-4972-8e13-0157d90e2977",
   "metadata": {},
   "source": [
    "Lets see the prediction to find out whether the model learned something or is just being smartass (guessing majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd24b3c-7bf0-4c32-9ac3-aebd5099dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b784e0-1609-45e9-9ee7-b5198694cb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
