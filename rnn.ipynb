{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb58543-b5d0-4dc0-8a2f-dbfb263899b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ffddf",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c845e911-d47e-4eb8-a5e6-34ed6852f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 18:56:48.480547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "\n",
    "from leetcode_dataset import LeetcodeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b850a356-2412-4a15-aa29-de76a2f9c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab4083",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89dc9eb-9ac8-4377-a2ad-b9c44c73484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = LeetcodeDataset().dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4d972",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66c6a17-1091-4a86-a5ce-251e243a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['explanation'])\n",
    "sequences = tokenizer.texts_to_sequences(df['explanation'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "y = df['difficulty int'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7870a21",
   "metadata": {},
   "source": [
    "Generate balanced batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5f785c-95dc-4e67-880d-d1649782df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train)=1460, len(X_val)=182, len(X_test)=183\n"
     ]
    }
   ],
   "source": [
    "# 80:10:10 train:validation:test\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_val\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = np.array(X_train), np.array(X_val), np.array(X_test), np.array(y_train), np.array(y_val), np.array(y_test)\n",
    "print(f\"{len(X_train)=}, {len(X_val)=}, {len(X_test)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b44cdda-f21c-4cf8-9a98-a2fe28a36a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_batches_bootstrap(X, y, batch_size, n_classes, n_batches):\n",
    "    for _ in range(n_batches):\n",
    "        X_batch = np.zeros((batch_size, max_length))\n",
    "        y_batch = np.zeros(batch_size)\n",
    "        \n",
    "        samples_per_class = batch_size // n_classes\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            class_indices = np.where(y == i)[0]\n",
    "            \n",
    "            chosen_indices = resample(class_indices, n_samples=samples_per_class, replace=True)\n",
    "            \n",
    "            X_batch[i * samples_per_class:(i + 1) * samples_per_class] = X[chosen_indices]\n",
    "            y_batch[i * samples_per_class:(i + 1) * samples_per_class] = y[chosen_indices]\n",
    "        \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49c7fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 18:56:59.062925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43683 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE, NUMBER_OF_BATCHES = 15, 10\n",
    "\n",
    "train = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_balanced_batches_bootstrap(X_train, y_train, BATCH_SIZE, 3, NUMBER_OF_BATCHES),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=([BATCH_SIZE, max_length], [BATCH_SIZE])\n",
    ")\n",
    "\n",
    "validation = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_balanced_batches_bootstrap(X_val, y_val, 5, 3, 5),\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=([5, max_length], [5])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa12fe48",
   "metadata": {},
   "source": [
    "## Build RNN\n",
    "\n",
    "Implement a function that creates dense layer with batchnorm + dropout regularizations. Parameters are feeded from hypersearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fd74fc6-2006-4fea-aed0-637709a6ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_block(input, units, dropout_rate, recurrent_dropout, hp_rnn_type, return_sequences=True):\n",
    "    if hp_rnn_type == 'lstm':\n",
    "        rnn_layer = tf.keras.layers.LSTM(units, dropout=dropout_rate, recurrent_dropout=recurrent_dropout, return_sequences=return_sequences)(input)\n",
    "    elif hp_rnn_type == 'gru':\n",
    "        rnn_layer = tf.keras.layers.GRU(units, dropout=dropout_rate, recurrent_dropout=recurrent_dropout, return_sequences=return_sequences)(input)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid RNN type\")\n",
    "\n",
    "    return rnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63e10f7-23cb-4c2b-bb48-66e76f2df4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input = tf.keras.layers.Input(shape=(max_length,))\n",
    "\n",
    "    hp_embedding_dim = hp.Int('embedding_dim', min_value=32, max_value=256, step=32)\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=hp_embedding_dim, mask_zero=True)(input)\n",
    "    # now 3d - (batch_size, sequence, embed_dim)\n",
    "    \n",
    "    hp_number_of_rnn_layers = hp.Int('number_of_rnn_layers', 1, 2)\n",
    "    rnn = embedding\n",
    "    for i in range(hp_number_of_rnn_layers):\n",
    "        hp_units = hp.Int(f'rnn_units_{i+1}', min_value=32, max_value=512, step=32)\n",
    "        hp_dropout = hp.Choice(f'rnn_dropout_{i+1}', values=[.2, .3, .4, .5])\n",
    "        hp_recurrent_dropout = hp.Choice(f'rnn_recurrent_dropout_{i+1}', values=[.2, .3, .4, .5])\n",
    "        hp_rnn_type = hp.Choice(f'rnn_type_{i+1}', values=['lstm', 'gru'])\n",
    "        rnn = rnn_block(rnn, units=hp_units, dropout_rate=hp_dropout, recurrent_dropout=hp_recurrent_dropout, hp_rnn_type=hp_rnn_type)\n",
    "\n",
    "    # return sequences = False will make 2d out of 3d - (batch_size, units)\n",
    "    rnn = rnn_block(rnn, units=hp_units, dropout_rate=hp_dropout, recurrent_dropout=hp_recurrent_dropout, hp_rnn_type=hp_rnn_type, return_sequences=False)\n",
    "\n",
    "    output = tf.keras.layers.Dense(units=3, activation=tf.nn.softmax)(rnn)\n",
    "    model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[.05, .01, .005, .001, .0005, .0001])\n",
    "    hp_sgd_momentum = hp.Choice('sgd_momentum', values=[.8, .9])\n",
    "    hp_optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
    "\n",
    "    if hp_optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer_name == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=hp_learning_rate, momentum=hp_sgd_momentum, nesterov=True)\n",
    "    elif hp_optimizer_name == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.experimental.RMSprop(learning_rate=hp_learning_rate)\n",
    "    elif hp_optimizer_name == 'adagrad':\n",
    "        optimizer = tf.keras.optimizers.experimental.Adagrad(learning_rate=hp_learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923de2-028f-48d6-9b6c-89948e215d43",
   "metadata": {},
   "source": [
    "Set up hyper search using the hyper band algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86850d6b-c5b4-4959-82ba-660765c42ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_sparse_categorical_accuracy',\n",
    "    directory='rnn_model',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ffac1c-4f75-44af-9304-c12d4d1a3c72",
   "metadata": {},
   "source": [
    "### Find best hyperparams for the MLP\n",
    "\n",
    "Start search for the best hyperparameters given the current datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0219b-65f1-426a-a08a-3abb3b678645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 153 Complete [00h 01m 25s]\n",
      "val_sparse_categorical_accuracy: 0.6000000238418579\n",
      "\n",
      "Best val_sparse_categorical_accuracy So Far: 0.8799999952316284\n",
      "Total elapsed time: 02h 11m 16s\n",
      "\n",
      "Search: Running Trial #154\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "224               |128               |embedding_dim\n",
      "2                 |2                 |number_of_rnn_layers\n",
      "128               |128               |rnn_units_1\n",
      "0.3               |0.4               |rnn_dropout_1\n",
      "0.5               |0.2               |rnn_recurrent_dropout_1\n",
      "lstm              |lstm              |rnn_type_1\n",
      "0.0005            |0.0005            |learning_rate\n",
      "0.8               |0.9               |sgd_momentum\n",
      "sgd               |adam              |optimizer\n",
      "288               |224               |rnn_units_2\n",
      "0.3               |0.4               |rnn_dropout_2\n",
      "0.3               |0.2               |rnn_recurrent_dropout_2\n",
      "gru               |gru               |rnn_type_2\n",
      "4                 |34                |tuner/epochs\n",
      "0                 |12                |tuner/initial_epoch\n",
      "3                 |4                 |tuner/bracket\n",
      "0                 |3                 |tuner/round\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.0981 - sparse_categorical_accuracy: 0.3533 - val_loss: 1.0989 - val_sparse_categorical_accuracy: 0.6400 - lr: 5.0000e-04\n",
      "Epoch 2/4\n",
      "10/10 [==============================] - 20s 2s/step - loss: 1.0984 - sparse_categorical_accuracy: 0.4067 - val_loss: 1.0981 - val_sparse_categorical_accuracy: 0.6800 - lr: 5.0000e-04\n",
      "Epoch 3/4\n",
      " 3/10 [========>.....................] - ETA: 12s - loss: 1.0991 - sparse_categorical_accuracy: 0.2222"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    train,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            patience=10\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e157af7-c03e-4a44-8d93-8b9e6932042e",
   "metadata": {},
   "source": [
    "Extract hyperparameters from the best trial and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d148b6-fdb6-4f67-ab07-81b24e45ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_hps = best_trial.hyperparameters\n",
    "\n",
    "with open('rnn_best_hp.json', 'w') as f:\n",
    "    json.dump(best_hps.values, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7868bc5-ef4d-4b90-91b8-53fbd557ef32",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Build the best model using the best hyperparameters, save the model and print its tensorflow summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e713a-f953-4ed8-b72a-d550114951dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(best_hps)\n",
    "best_model.save('rnn_best.h5')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3831ac-ca52-48b6-bba3-542ef9b21b11",
   "metadata": {},
   "source": [
    "Lets see the loss and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1387138-202b-4399-9ddd-560b7e208ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7da75-6154-4972-8e13-0157d90e2977",
   "metadata": {},
   "source": [
    "Lets see the prediction to find out whether the model learned something or is just being smartass (guessing majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd24b3c-7bf0-4c32-9ac3-aebd5099dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b784e0-1609-45e9-9ee7-b5198694cb44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
